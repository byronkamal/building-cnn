{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.0.0\n",
      "  Using cached https://files.pythonhosted.org/packages/b9/88/f6b026a424d66d185534cb356fecaa63c96540227c306b2d96b61385f8d1/tensorflow-2.0.0-cp35-cp35m-manylinux2010_x86_64.whl\n",
      "Collecting numpy==1.17.3\n",
      "  Using cached https://files.pythonhosted.org/packages/5e/f8/82a8a6ed446b58aa718b2744b265983783a2c84098a73db6d0b78a573e25/numpy-1.17.3-cp35-cp35m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: pandas==0.24.2 in /home/guiz/unb/dl-unb/dl-unb/lib/python3.5/site-packages (from -r requirements.txt (line 3)) (0.24.2)\n",
      "Requirement already satisfied: matplotlib==3.0.3 in /home/guiz/unb/dl-unb/dl-unb/lib/python3.5/site-packages (from -r requirements.txt (line 4)) (3.0.3)\n",
      "Collecting Keras==2.3.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl (377kB)\n",
      "\u001b[K     |████████████████████████████████| 378kB 751kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: sklearn==0.0 in /home/guiz/unb/dl-unb/dl-unb/lib/python3.5/site-packages (from -r requirements.txt (line 6)) (0.0)\n",
      "Collecting pillow==6.2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/34/086feba718f629fc51f27f993fa72e210c1c25077cba2f9e9a2dcfa23a7f/Pillow-6.2.0-cp35-cp35m-manylinux1_x86_64.whl (2.1MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1MB 12.2MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /home/guiz/unb/dl-unb/dl-unb/lib/python3.5/site-packages (from tensorflow==2.0.0->-r requirements.txt (line 1)) (1.1.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /home/guiz/unb/dl-unb/dl-unb/lib/python3.5/site-packages (from tensorflow==2.0.0->-r requirements.txt (line 1)) (3.9.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /home/guiz/unb/dl-unb/dl-unb/lib/python3.5/site-packages (from tensorflow==2.0.0->-r requirements.txt (line 1)) (0.1.7)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/guiz/unb/dl-unb/dl-unb/lib/python3.5/site-packages (from tensorflow==2.0.0->-r requirements.txt (line 1)) (1.11.2)\n",
      "Processing /home/guiz/.cache/pip/wheels/2c/b1/94/43d03e130b929aae7ba3f8d15cbd7bc0d1cb5bb38a5c721833/opt_einsum-3.1.0-cp35-none-any.whl\n",
      "Processing /home/guiz/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd/gast-0.2.2-cp35-none-any.whl\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /home/guiz/unb/dl-unb/dl-unb/lib/python3.5/site-packages (from tensorflow==2.0.0->-r requirements.txt (line 1)) (1.0.8)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/guiz/unb/dl-unb/dl-unb/lib/python3.5/site-packages (from tensorflow==2.0.0->-r requirements.txt (line 1)) (1.24.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /home/guiz/unb/dl-unb/dl-unb/lib/python3.5/site-packages (from tensorflow==2.0.0->-r requirements.txt (line 1)) (0.8.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /home/guiz/unb/dl-unb/dl-unb/lib/python3.5/site-packages (from tensorflow==2.0.0->-r requirements.txt (line 1)) (0.8.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/guiz/unb/dl-unb/dl-unb/lib/python3.5/site-packages (from tensorflow==2.0.0->-r requirements.txt (line 1)) (1.12.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/guiz/unb/dl-unb/dl-unb/lib/python3.5/site-packages (from tensorflow==2.0.0->-r requirements.txt (line 1)) (1.1.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/guiz/unb/dl-unb/dl-unb/lib/python3.5/site-packages (from tensorflow==2.0.0->-r requirements.txt (line 1)) (0.33.6)\n",
      "Collecting tensorflow-estimator<2.1.0,>=2.0.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n",
      "\u001b[K     |████████████████████████████████| 450kB 4.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard<2.1.0,>=2.0.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/9e/a48cd34dd7b672ffc227b566f7d16d63c62c58b542d54efa45848c395dd4/tensorboard-2.0.1-py3-none-any.whl (3.8MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8MB 178kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.5.0 in /home/guiz/unb/dl-unb/dl-unb/lib/python3.5/site-packages (from pandas==0.24.2->-r requirements.txt (line 3)) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2011k in /home/guiz/unb/dl-unb/dl-unb/lib/python3.5/site-packages (from pandas==0.24.2->-r requirements.txt (line 3)) (2019.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/guiz/unb/dl-unb/dl-unb/lib/python3.5/site-packages (from matplotlib==3.0.3->-r requirements.txt (line 4)) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/guiz/unb/dl-unb/dl-unb/lib/python3.5/site-packages (from matplotlib==3.0.3->-r requirements.txt (line 4)) (2.4.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/guiz/unb/dl-unb/dl-unb/lib/python3.5/site-packages (from matplotlib==3.0.3->-r requirements.txt (line 4)) (1.1.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/guiz/unb/dl-unb/dl-unb/lib/python3.5/site-packages (from Keras==2.3.1->-r requirements.txt (line 5)) (1.3.1)\n",
      "Requirement already satisfied: h5py in /home/guiz/unb/dl-unb/dl-unb/lib/python3.5/site-packages (from Keras==2.3.1->-r requirements.txt (line 5)) (2.10.0)\n",
      "Requirement already satisfied: pyyaml in /home/guiz/unb/dl-unb/dl-unb/lib/python3.5/site-packages (from Keras==2.3.1->-r requirements.txt (line 5)) (5.1.2)\n",
      "Requirement already satisfied: scikit-learn in /home/guiz/unb/dl-unb/dl-unb/lib/python3.5/site-packages (from sklearn==0.0->-r requirements.txt (line 6)) (0.21.3)\n",
      "Requirement already satisfied: setuptools in /home/guiz/unb/dl-unb/dl-unb/lib/python3.5/site-packages (from protobuf>=3.6.1->tensorflow==2.0.0->-r requirements.txt (line 1)) (41.2.0)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading https://files.pythonhosted.org/packages/7b/b8/88def36e74bee9fce511c9519571f4e485e890093ab7442284f4ffaef60b/google_auth_oauthlib-0.4.1-py2.py3-none-any.whl\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/cb/786dc53d93494784935a62947643b48250b84a882474e714f9af5e1a1928/google_auth-1.7.1-py2.py3-none-any.whl (74kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 1.5MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: markdown>=2.6.8 in /home/guiz/unb/dl-unb/dl-unb/lib/python3.5/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->-r requirements.txt (line 1)) (3.1.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/guiz/unb/dl-unb/dl-unb/lib/python3.5/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0->-r requirements.txt (line 1)) (0.16.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/guiz/unb/dl-unb/dl-unb/lib/python3.5/site-packages (from scikit-learn->sklearn==0.0->-r requirements.txt (line 6)) (0.13.2)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/12/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379/requests_oauthlib-1.3.0-py2.py3-none-any.whl\n",
      "Collecting cachetools<3.2,>=2.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/2f/a6/30b0a0bef12283e83e58c1d6e7b5aabc7acfc4110df81a4471655d33e704/cachetools-3.1.1-py2.py3-none-any.whl\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/50/bb4cefca37da63a0c52218ba2cb1b1c36110d84dcbae8aa48cd67c5e95c2/pyasn1_modules-0.2.7-py2.py3-none-any.whl (131kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 7.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting rsa<4.1,>=3.1.4\n",
      "  Downloading https://files.pythonhosted.org/packages/02/e5/38518af393f7c214357079ce67a317307936896e961e35450b70fad2a9cf/rsa-4.0-py2.py3-none-any.whl\n",
      "Collecting oauthlib>=3.0.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/05/57/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704/oauthlib-3.1.0-py2.py3-none-any.whl (147kB)\n",
      "\u001b[K     |████████████████████████████████| 153kB 10.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests>=2.0.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl (57kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 61kB 139kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting pyasn1<0.5.0,>=0.4.6\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/1e/a94a8d635fa3ce4cfc7f506003548d0a2447ae76fd5ca53932970fe3053f/pyasn1-0.4.8-py2.py3-none-any.whl (77kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 45kB/s  eta 0:00:011\n",
      "\u001b[?25hCollecting idna<2.9,>=2.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl (58kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 87kB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting chardet<3.1.0,>=3.0.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl (133kB)\n",
      "\u001b[K     |████████████████████████████████| 143kB 11.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/40/a9837291310ee1ccc242ceb6ebfd9eb21539649f193a7c8c86ba15b98539/urllib3-1.25.7-py2.py3-none-any.whl (125kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 7.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting certifi>=2017.4.17\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/b0/8146a4f8dd402f60744fa380bc73ca47303cccf8b9190fd16a827281eac2/certifi-2019.9.11-py2.py3-none-any.whl (154kB)\n",
      "\u001b[K     |████████████████████████████████| 163kB 9.5MB/s eta 0:00:01\n",
      "\u001b[31mERROR: tensorboard 2.0.1 has requirement grpcio>=1.24.3, but you'll have grpcio 1.24.0 which is incompatible.\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy, opt-einsum, gast, tensorflow-estimator, oauthlib, idna, chardet, urllib3, certifi, requests, requests-oauthlib, cachetools, pyasn1, pyasn1-modules, rsa, google-auth, google-auth-oauthlib, tensorboard, tensorflow, Keras, pillow\n",
      "  Found existing installation: numpy 1.16.2\n",
      "    Uninstalling numpy-1.16.2:\n",
      "      Successfully uninstalled numpy-1.16.2\n",
      "  Found existing installation: gast 0.3.2\n",
      "    Uninstalling gast-0.3.2:\n",
      "      Successfully uninstalled gast-0.3.2\n",
      "  Found existing installation: tensorflow-estimator 1.13.0\n",
      "    Uninstalling tensorflow-estimator-1.13.0:\n",
      "      Successfully uninstalled tensorflow-estimator-1.13.0\n",
      "  Found existing installation: tensorboard 1.13.1\n",
      "    Uninstalling tensorboard-1.13.1:\n",
      "      Successfully uninstalled tensorboard-1.13.1\n",
      "  Found existing installation: tensorflow 1.13.1\n",
      "    Uninstalling tensorflow-1.13.1:\n",
      "      Successfully uninstalled tensorflow-1.13.1\n",
      "  Found existing installation: Keras 2.2.4\n",
      "    Uninstalling Keras-2.2.4:\n",
      "      Successfully uninstalled Keras-2.2.4\n",
      "Successfully installed Keras-2.3.1 cachetools-3.1.1 certifi-2019.9.11 chardet-3.0.4 gast-0.2.2 google-auth-1.7.1 google-auth-oauthlib-0.4.1 idna-2.8 numpy-1.17.3 oauthlib-3.1.0 opt-einsum-3.1.0 pillow-6.2.0 pyasn1-0.4.8 pyasn1-modules-0.2.7 requests-2.22.0 requests-oauthlib-1.3.0 rsa-4.0 tensorboard-2.0.1 tensorflow-2.0.0 tensorflow-estimator-2.0.1 urllib3-1.25.7\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Neural Network\n",
    "\n",
    "# Part 1 - Building the CNN\n",
    "\n",
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Step 1 - Convolution\n",
    "classifier.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'relu'))\n",
    "\n",
    "# Step 2 - Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adding a second convolutional layer\n",
    "classifier.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adding a second convolutional layer\n",
    "classifier.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Step 3 - Flattening\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Step 4 - Full connection\n",
    "classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "classifier.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the CNN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-583ad0f1c998>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m                                             class_mode = 'binary')\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m classifier.fit_generator(training_set,\n\u001b[0m\u001b[1;32m     23\u001b[0m                          \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                          \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'classifier' is not defined"
     ]
    }
   ],
   "source": [
    "# Part 2 - Fitting the CNN to the images\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('dataset/training_set',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('dataset/test_set',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')\n",
    "\n",
    "classifier.fit_generator(training_set,\n",
    "                         steps_per_epoch = 8000,\n",
    "                         epochs = 25,\n",
    "                         validation_data = test_set,\n",
    "                         validation_steps = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import PIL.Image. The use of `load_img` requires PIL.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a0e09a53f8bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtest_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dataset/single_prediction/cat_or_dog_1.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtest_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtest_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/unb/dl-unb/dl-unb/lib/python3.5/site-packages/keras_preprocessing/image/utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mcolor_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'grayscale'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpil_image\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         raise ImportError('Could not import PIL.Image. '\n\u001b[0m\u001b[1;32m    109\u001b[0m                           'The use of `load_img` requires PIL.')\n\u001b[1;32m    110\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Could not import PIL.Image. The use of `load_img` requires PIL."
     ]
    }
   ],
   "source": [
    "# Part 3 - Making new predictions\n",
    "\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('dataset/single_prediction/cat_or_dog_1.jpg', target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = classifier.predict(test_image)\n",
    "training_set.class_indices\n",
    "if result[0][0] == 1:\n",
    "    prediction = 'dog'\n",
    "else:\n",
    "    prediction = 'cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
